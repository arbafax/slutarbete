{
  "created_at": "2025-11-11T15:22:58.406418Z",
  "source_file": "chattbot.pdf",
  "page_count": 18,
  "chunk_count": 244,
  "paragraphs": [
    {
      "paragraph_id": 1,
      "page_num": 1,
      "paragraph_text": "Kapitel 10"
    },
    {
      "paragraph_id": 2,
      "page_num": 1,
      "paragraph_text": "Chattbottar"
    },
    {
      "paragraph_id": 3,
      "page_num": 1,
      "paragraph_text": "I detta kapitel kommer vi lära oss mer om hur chattbottar såsom ChatGPT fungerar och hur man ställer effektiva frågor till dem, vilket kallas prompt engineering. Vi kommer därefter lära oss hur vi bygger en lokal chattbot som vi kan använda på vår egna dator.\nKapitlet avslutas med att gå igenom Retrieval Augmented Generation (RAG) vilket låter oss anpassa chattbottens svar utifrån en given kontext, exempelvis utifrån egna dokument."
    },
    {
      "paragraph_id": 4,
      "page_num": 1,
      "paragraph_text": "10.1 ChatGPT och prompt engineering"
    },
    {
      "paragraph_id": 5,
      "page_num": 1,
      "paragraph_text": "Chattbottar, som till exempel ChatGPT, används av många människor för en rad olika syften. Alltifrån att skapa kreativt innehåll, få förslag på förbättringar av skriven text eller programmeringskod till att lösa mer komplexa problem."
    },
    {
      "paragraph_id": 6,
      "page_num": 1,
      "paragraph_text": "I korthet har chattbottar tränats på enorma datamängder och har från detta lärt sig vad som är rimliga sekvenser av ord. Exempelvis vet vi att om någon säger “Hej, hur mår ___” så brukar det sista ordet vara “du” för att meningen ska bli “Hej, hur mår du”."
    },
    {
      "paragraph_id": 7,
      "page_num": 1,
      "paragraph_text": "När vi ställer en fråga till ChatGPT, uppskattar den sannolikheter för nästkommande ord och ger alltså ett svar som är sannolikt utifrån den datan som den blivit tränad på. I korthet kan vi säga att det fungerar som en “stokastisk papegoja”. Stokastisk för att den uppskattar sannolikheter och papegoja för att den upprepar vad den lärt sig baserat på datan den blivit tränad på."
    },
    {
      "paragraph_id": 8,
      "page_num": 1,
      "paragraph_text": "329"
    },
    {
      "paragraph_id": 9,
      "page_num": 2,
      "paragraph_text": "Det är uppenbart att AI-modeller och svaren de ger är beroende av datan de tränats på och alltså inte “sanna” i en egentlig mening. Läsaren uppmanas till att tänka igenom vilka etiska implikationer och risker detta kan ha."
    },
    {
      "paragraph_id": 10,
      "page_num": 2,
      "paragraph_text": "Den läsare som vill fördjupa sig i hur ChatGPT funkar kan läsa igenom följande relativt korta bok: “What Is ChatGPT Doing … and Why Does It Work?” av Stephen Wol- fram. Boken finns tillgänglig här: https://writings.stephenwolfram.com/2023/02/what- is-chatgpt-doing-and-why-does-it-work/."
    },
    {
      "paragraph_id": 11,
      "page_num": 2,
      "paragraph_text": "Ett gammalt talesätt säger “som man frågar får man svar”. Detta gäller även chattbottar.\nGenerellt sett kan vi ställa de frågor vi har och få rimliga svar utan att behöva tänka alltför mycket. Om vi däremot vill ha bättre svar kan vi försöka ställa effektiva frågor.\nHur detta rent praktiskt går till är temat för det som kallas för prompt engineering.\nOlika chattbottar kan fungera på olika sätt men som tumregel är det bra att vara så specifik, deskriptiv och detaljerad som möjligt om önskad kontext, utfall, längd, format, stil med mera. Om vi jämför de två prompterna nedan,"
    },
    {
      "paragraph_id": 12,
      "page_num": 2,
      "paragraph_text": "“skriv en dikt om AI”."
    },
    {
      "paragraph_id": 13,
      "page_num": 2,
      "paragraph_text": "och"
    },
    {
      "paragraph_id": 14,
      "page_num": 2,
      "paragraph_text": "“Skriv en kort och inspirerande dikt om AI som fokuserar på dess möjligheter och risker inom utbildning. Stilen ska likna Shakespears.”"
    },
    {
      "paragraph_id": 15,
      "page_num": 2,
      "paragraph_text": "är den andra mer effektiv."
    },
    {
      "paragraph_id": 16,
      "page_num": 2,
      "paragraph_text": "Den läsare som vill lära sig mer om prompt engineering kan exempelvis kolla på föl- jande guide kopplad till ChatGPT: https://help.openai.com/en/articles/6654000-best- practices-for-prompt-engineering-with-the-openai-api."
    },
    {
      "paragraph_id": 17,
      "page_num": 2,
      "paragraph_text": "10.2 Molnbaserade och lokala språkmodeller"
    },
    {
      "paragraph_id": 18,
      "page_num": 2,
      "paragraph_text": "Att träna en Large Language Model (LLM) kostar både tid och pengar och kräver stora mängder energi. För de allra flesta är det därför bättre att använda en förtränad modell.\nDet kan vi åstadkomma antingen genom att koppla upp mot en modell i molnet via ett API, eller genom att ladda ned en förtränad modell och köra den lokalt på vår egna dator."
    },
    {
      "paragraph_id": 19,
      "page_num": 2,
      "paragraph_text": "Nedan beskrivs några för- och nackdelar med respektive tillvägagångssätt och därefter demonstreras hur vi skapar en chattbot som kopplar upp mot en modell i molnet."
    },
    {
      "paragraph_id": 20,
      "page_num": 2,
      "paragraph_text": "330"
    },
    {
      "paragraph_id": 21,
      "page_num": 3,
      "paragraph_text": "10.2.1 Molnbaserade modeller"
    },
    {
      "paragraph_id": 22,
      "page_num": 3,
      "paragraph_text": "Att använda sig av molnbaserade modeller har sina för- och nackdelar."
    },
    {
      "paragraph_id": 23,
      "page_num": 3,
      "paragraph_text": "Fördelar"
    },
    {
      "paragraph_id": 24,
      "page_num": 3,
      "paragraph_text": "• Enkla att implementera i ett befintligt system tack vare API • Skalar automatiskt efter behov • Uppdateras i takt med att teknologin utvecklas"
    },
    {
      "paragraph_id": 25,
      "page_num": 3,
      "paragraph_text": "Nackdelar"
    },
    {
      "paragraph_id": 26,
      "page_num": 3,
      "paragraph_text": "• Ökad användning kan leda till ökade kostnader • Risker med hantering av känslig data • Kräver extern anslutning till internet, hastigheten kan påverkas av problem med nätverket"
    },
    {
      "paragraph_id": 27,
      "page_num": 3,
      "paragraph_text": "10.2.2 Lokala modeller"
    },
    {
      "paragraph_id": 28,
      "page_num": 3,
      "paragraph_text": "Även lokala modeller har sina för- och nackdelar."
    },
    {
      "paragraph_id": 29,
      "page_num": 3,
      "paragraph_text": "Fördelar"
    },
    {
      "paragraph_id": 30,
      "page_num": 3,
      "paragraph_text": "• Större kontroll över känslig data • Kan tränas ytterligare för att skapa specialiserade modeller • Svarar snabbare och kräver ingen extern anslutning till internet"
    },
    {
      "paragraph_id": 31,
      "page_num": 3,
      "paragraph_text": "Nackdelar"
    },
    {
      "paragraph_id": 32,
      "page_num": 3,
      "paragraph_text": "• Investeringar i infrastruktur kan innebära höga kostnader initialt • Uppskalning kan innebära ytterligare investeringar"
    },
    {
      "paragraph_id": 33,
      "page_num": 3,
      "paragraph_text": "En stor samling av modeller finns tillgängliga på Hugging Face: https://huggingface.co, som också fungerar som ett community för utveckling och implementering av AI."
    },
    {
      "paragraph_id": 34,
      "page_num": 3,
      "paragraph_text": "10.3 Bygga en chattbot"
    },
    {
      "paragraph_id": 35,
      "page_num": 3,
      "paragraph_text": "I följande avsnitt ska vi steg för steg bygga en enkel chattbot som körs i terminalen. Vi kommer att använda Googles Gemini-modell som LLM."
    },
    {
      "paragraph_id": 36,
      "page_num": 3,
      "paragraph_text": "331"
    },
    {
      "paragraph_id": 37,
      "page_num": 4,
      "paragraph_text": "Ĺ För att kunna använda Gemini behöver vi en API-nyckel, vilket vi kan få genom att logga in på https://aistudio.google.com/ och skapa en nyckel."
    },
    {
      "paragraph_id": 38,
      "page_num": 4,
      "paragraph_text": "För att inte dela med sig av sin privata API-nyckel brukar man vanligtvis spara den som en miljövariabel och läsa in den med os.getenv()."
    },
    {
      "paragraph_id": 39,
      "page_num": 4,
      "paragraph_text": "Vi installerar biblioteket google-genai via pip. Vi kan också använda exempelvis conda eller uv."
    },
    {
      "paragraph_id": 40,
      "page_num": 4,
      "paragraph_text": "> pip install google-genai"
    },
    {
      "paragraph_id": 41,
      "page_num": 4,
      "paragraph_text": "Vi börjar med att kolla så att vi kan få Gemini att generera svar."
    },
    {
      "paragraph_id": 42,
      "page_num": 4,
      "paragraph_text": "import os from google import genai"
    },
    {
      "paragraph_id": 43,
      "page_num": 4,
      "paragraph_text": "client = genai.Client(api_key=os.getenv(\"API_KEY\")) 1"
    },
    {
      "paragraph_id": 44,
      "page_num": 4,
      "paragraph_text": "1 Skapa en instans av Client-klassen från genai-biblioteket."
    },
    {
      "paragraph_id": 45,
      "page_num": 4,
      "paragraph_text": "response = client.models.generate_content( 1 model=\"gemini-2.0-flash\", contents=\"Hej där, vem pratar jag med?\" )"
    },
    {
      "paragraph_id": 46,
      "page_num": 4,
      "paragraph_text": "print(response.text)"
    },
    {
      "paragraph_id": 47,
      "page_num": 4,
      "paragraph_text": "1 Generera ett svar med generate_content()-metoden från vår client-instans."
    },
    {
      "paragraph_id": 48,
      "page_num": 4,
      "paragraph_text": "Hej! Du pratar med en stor språkmodell, tränad av Google. Du kan kalla mig för Google AI, eller bara en AI. Vad kan jag hjälpa dig med?"
    },
    {
      "paragraph_id": 49,
      "page_num": 4,
      "paragraph_text": "Med hjälp av en while-loop och Pythons input()-funktion kan vi nu skapa en enkel textbaserad chattbot som vi kan köra i terminalen. Koden nedan demonstrerar detta och Figur 10.1 visar hur det kan se ut."
    },
    {
      "paragraph_id": 50,
      "page_num": 4,
      "paragraph_text": "# chatbot.py"
    },
    {
      "paragraph_id": 51,
      "page_num": 4,
      "paragraph_text": "332"
    },
    {
      "paragraph_id": 52,
      "page_num": 5,
      "paragraph_text": "import os from google import genai"
    },
    {
      "paragraph_id": 53,
      "page_num": 5,
      "paragraph_text": "client = genai.Client(api_key=os.getenv(\"API_KEY\"))"
    },
    {
      "paragraph_id": 54,
      "page_num": 5,
      "paragraph_text": "if __name__ == \"__main__\":"
    },
    {
      "paragraph_id": 55,
      "page_num": 5,
      "paragraph_text": "print(\"*** Gemini chat ***\") print(\"Type <q> to exit chat.\") while True:"
    },
    {
      "paragraph_id": 56,
      "page_num": 5,
      "paragraph_text": "prompt = input(\"User: \") if prompt == \"q\":"
    },
    {
      "paragraph_id": 57,
      "page_num": 5,
      "paragraph_text": "break else:"
    },
    {
      "paragraph_id": 58,
      "page_num": 5,
      "paragraph_text": "response = client.models.generate_content("
    },
    {
      "paragraph_id": 59,
      "page_num": 5,
      "paragraph_text": "model=\"gemini-2.0-flash\", contents=prompt ) print(\"Gemini: \" + response.text)"
    },
    {
      "paragraph_id": 60,
      "page_num": 5,
      "paragraph_text": "Figur 10.1: En enkel chattbot i terminalen."
    },
    {
      "paragraph_id": 61,
      "page_num": 5,
      "paragraph_text": "10.4 Retrieval Augmented Generation (RAG)"
    },
    {
      "paragraph_id": 62,
      "page_num": 5,
      "paragraph_text": "Om vi vill att vår Gemini-chattbot ska hålla sig till ett visst ämne kan vi använda en teknik som kallas RAG (Retrieval Augmented Generation). Det går ut på att vi ger modellen en kontext, ofta ett eller flera dokument eller liknande, att förhålla sig till.\nI en prompt säger vi åt modellen att enbart svara utifrån den givna kontexten. Om modellen inte hittar svaret i kontexten ska den säga det istället för att försöka hitta svaren någon annanstans eller gissa."
    },
    {
      "paragraph_id": 63,
      "page_num": 5,
      "paragraph_text": "En RAG-modell innehåller alltså två delar."
    },
    {
      "paragraph_id": 64,
      "page_num": 5,
      "paragraph_text": "333"
    },
    {
      "paragraph_id": 65,
      "page_num": 6,
      "paragraph_text": "Den första delen är en retriever, som söker efter relevanta stycken i en större text.\nDessa stycken skickas sedan vidare som kontext till den andra delen, en generator som genererar svaren utifrån den givna kontexten."
    },
    {
      "paragraph_id": 66,
      "page_num": 6,
      "paragraph_text": "För att ge modellen en kontext behöver vi läsa in data (exempelvis ett eller flera PDF- dokument) och bearbeta den så att vi kan göra en semantisk sökning i datan, det vill säga leta upp de stycken i kontexten som verkar ha mest med själva frågan att göra.\nDessa stycken skickar vi sedan med till språkmodellen när vi ställer vår fråga."
    },
    {
      "paragraph_id": 67,
      "page_num": 6,
      "paragraph_text": "Det finns ett antal ramverk för att implementera RAG, bland annat LangChain. Vi kommer inte använda något ramverk i det här kapitlet utan istället använda oss av vanligt förekommande Python-bibliotek som pypdf för att läsa in text från ett PDF- dokument och numpy för beräkningar samt en enkel vector store för att hantera vår data."
    },
    {
      "paragraph_id": 68,
      "page_num": 6,
      "paragraph_text": "I resten av det här kapitlet kommer vi att använda texten från det här kapitlet som kontext när vi ställer frågor till vår chattbot."
    },
    {
      "paragraph_id": 69,
      "page_num": 6,
      "paragraph_text": "Kodexemplet som följer bygger på kod från ett Github-repository som är en myc- ket bra källa till vidare läsning och experimentering. Repositoryt kan hittas på https://github.com/FareedKhan-dev/all-rag-techniques."
    },
    {
      "paragraph_id": 70,
      "page_num": 6,
      "paragraph_text": "10.4.1 Läs in data"
    },
    {
      "paragraph_id": 71,
      "page_num": 6,
      "paragraph_text": "RAG-tekniken kan användas med många olika typer av data. I det här exemplet kommer vi arbeta med text-data."
    },
    {
      "paragraph_id": 72,
      "page_num": 6,
      "paragraph_text": "Vi läser in ett PDF-dokument och extraherar all text i dokumentet."
    },
    {
      "paragraph_id": 73,
      "page_num": 6,
      "paragraph_text": "from pypdf import PdfReader"
    },
    {
      "paragraph_id": 74,
      "page_num": 6,
      "paragraph_text": "reader = PdfReader(\"chattbot.pdf\")"
    },
    {
      "paragraph_id": 75,
      "page_num": 6,
      "paragraph_text": "text = \"\""
    },
    {
      "paragraph_id": 76,
      "page_num": 6,
      "paragraph_text": "for page in reader.pages:"
    },
    {
      "paragraph_id": 77,
      "page_num": 6,
      "paragraph_text": "text += page.extract_text()"
    },
    {
      "paragraph_id": 78,
      "page_num": 6,
      "paragraph_text": "334"
    },
    {
      "paragraph_id": 79,
      "page_num": 7,
      "paragraph_text": "10.4.2 Chunking"
    },
    {
      "paragraph_id": 80,
      "page_num": 7,
      "paragraph_text": "Just nu är vår variabel text en enda lång textsträng som innehåller hela kapitlet. Vi behöver dela upp den i mindre delar, som brukar kallas chunks. Det är för att vi så småningom kommer att söka efter de delar av texten som innehåller den information vi är intresserade av, så kallad semantisk sökning."
    },
    {
      "paragraph_id": 81,
      "page_num": 7,
      "paragraph_text": "Det finns ett antal olika strategier för chunking. Vi kommer inte gå igenom dem alla i det här kapitlet utan fokuserar på några av de vanligaste: fixed-length chunking, sentence- based chunking, och semantic chunking."
    },
    {
      "paragraph_id": 82,
      "page_num": 7,
      "paragraph_text": "Fixed-length chunking"
    },
    {
      "paragraph_id": 83,
      "page_num": 7,
      "paragraph_text": "En vanlig strategi är den som kallas fixed-length chunking. Vi delar då upp texten i ett antal lika stora delar baserat på tokens, ord eller tecken. Det är en effektiv och enkel strategi, men den görs utan hänsyn till innehållet i texten. Det kan leda till att vi tappar information och att våra semantiska sökningar blir sämre."
    },
    {
      "paragraph_id": 84,
      "page_num": 7,
      "paragraph_text": "När vi använder fixed-length chunking låter vi vanligtvis delarna överlappa med ett antal tecken. Hur stora delarna ska vara och hur många tecken som ska överlappa beror på typen av text och är något vi kan behöva prova oss fram till när vi bygger en RAG- modell."
    },
    {
      "paragraph_id": 85,
      "page_num": 7,
      "paragraph_text": "I kodexemplet nedan skapar vi chunks som innehåller 1000 tecken vardera, med 200 teckens överlappning mellan delarna."
    },
    {
      "paragraph_id": 86,
      "page_num": 7,
      "paragraph_text": "chunks = [] n = 1000 overlap = 200"
    },
    {
      "paragraph_id": 87,
      "page_num": 7,
      "paragraph_text": "for i in range(0, len(text), n - overlap):"
    },
    {
      "paragraph_id": 88,
      "page_num": 7,
      "paragraph_text": "chunks.append(text[i:i + n])"
    },
    {
      "paragraph_id": 89,
      "page_num": 7,
      "paragraph_text": "print(f\"Antal chunks: {len(chunks)}.\")"
    },
    {
      "paragraph_id": 90,
      "page_num": 7,
      "paragraph_text": "Antal chunks: 30."
    },
    {
      "paragraph_id": 91,
      "page_num": 7,
      "paragraph_text": "Sentence-based chunking"
    },
    {
      "paragraph_id": 92,
      "page_num": 7,
      "paragraph_text": "Ett alternativ till fixed-length chunking är sentence-based chunking. Vi delar upp texten i meningar, till exempel genom Pythons split()-funktion."
    },
    {
      "paragraph_id": 93,
      "page_num": 7,
      "paragraph_text": "335"
    },
    {
      "paragraph_id": 94,
      "page_num": 8,
      "paragraph_text": "sentences = text.split(\". \")"
    },
    {
      "paragraph_id": 95,
      "page_num": 8,
      "paragraph_text": "print(f\"Antal meningar: {len(sentences)}.\")"
    },
    {
      "paragraph_id": 96,
      "page_num": 8,
      "paragraph_text": "Antal meningar: 62."
    },
    {
      "paragraph_id": 97,
      "page_num": 8,
      "paragraph_text": "Vi återkommer till semantic chunking längre fram."
    },
    {
      "paragraph_id": 98,
      "page_num": 8,
      "paragraph_text": "I det här kapitlet kommer vi använda fixed-length chunking."
    },
    {
      "paragraph_id": 99,
      "page_num": 8,
      "paragraph_text": "10.4.3 Embeddings"
    },
    {
      "paragraph_id": 100,
      "page_num": 8,
      "paragraph_text": "Vi behöver dock köra vår textdata genom ytterligare ett steg innan den är redo att användas som kontext till vår RAG-modell. Vi behöver skapa embeddings, det vill säga numeriska representationer av texten. Embeddings fångar textens betydelse, vilket gör att chunks som ligger närmare varandra i betydelse också har embeddings som ligger närmare varandra rent matematiskt."
    },
    {
      "paragraph_id": 101,
      "page_num": 8,
      "paragraph_text": "Vi definierar en funktion create_embeddings(), som använder embed_content()- metoden från vår client-instans."
    },
    {
      "paragraph_id": 102,
      "page_num": 8,
      "paragraph_text": "from google.genai import types"
    },
    {
      "paragraph_id": 103,
      "page_num": 8,
      "paragraph_text": "def create_embeddings(text,"
    },
    {
      "paragraph_id": 104,
      "page_num": 8,
      "paragraph_text": "model=\"text-embedding-004\", task_type=\"SEMANTIC_SIMILARITY\"):\n1 return client.models.embed_content( 2 model=model, contents=text, config=types.EmbedContentConfig(task_type=task_type))"
    },
    {
      "paragraph_id": 105,
      "page_num": 8,
      "paragraph_text": "1 Ange text, vilken modell vi vill använda samt i vilket syfte vi vill skapa embeddings."
    },
    {
      "paragraph_id": 106,
      "page_num": 8,
      "paragraph_text": "2 Funktionen kör i sin tur metoden embed_content()."
    },
    {
      "paragraph_id": 107,
      "page_num": 8,
      "paragraph_text": "embeddings = create_embeddings(chunks)"
    },
    {
      "paragraph_id": 108,
      "page_num": 8,
      "paragraph_text": "Våra embeddings lagras som vektorer. I Avsnitt 10.5 skapar vi en enkel vector store för att hantera våra chunks och embeddings."
    },
    {
      "paragraph_id": 109,
      "page_num": 8,
      "paragraph_text": "336"
    },
    {
      "paragraph_id": 110,
      "page_num": 9,
      "paragraph_text": "10.4.4 Semantisk sökning"
    },
    {
      "paragraph_id": 111,
      "page_num": 9,
      "paragraph_text": "När vi har skapat våra embeddings är vi redo att göra semantiska sökningar för att hitta de chunks som verkar ligga närmast vår fråga i betydelse."
    },
    {
      "paragraph_id": 112,
      "page_num": 9,
      "paragraph_text": "Det finns lite olika algoritmer vi kan använda oss av när vi jämför texter i en semantisk sökning. En av de vanligaste är cosinuslikheten (cosine similarity), som beskriver vinkeln mellan två vektorer. Cosinuslikheten är ett tal i intervallet [−1, 1]. Figur 10.2 visar exempel på tre olika par av vektorer med olika grad av cosinuslikhet:"
    },
    {
      "paragraph_id": 113,
      "page_num": 9,
      "paragraph_text": "• proportionella med cosinuslikheten 1 • ortogonala (vinkelräta) med cosinuslikheten 0 • motsatta med cosinuslikheten -1."
    },
    {
      "paragraph_id": 114,
      "page_num": 9,
      "paragraph_text": "Notera att de vektorer som utgörs av våra embeddings har många fler dimensioner än de som visas i Figur 10.2."
    },
    {
      "paragraph_id": 115,
      "page_num": 9,
      "paragraph_text": "(a) Två proportionella vektorer (b) Två ortogonala vektorer (c) Två motsatta vektorer"
    },
    {
      "paragraph_id": 116,
      "page_num": 9,
      "paragraph_text": "Figur 10.2: Exempel på cosinuslikheter."
    },
    {
      "paragraph_id": 117,
      "page_num": 9,
      "paragraph_text": "Vi kan skriva en funktion som räknar ut cosinuslikheten enligt exemplet nedan."
    },
    {
      "paragraph_id": 118,
      "page_num": 9,
      "paragraph_text": "import numpy as np"
    },
    {
      "paragraph_id": 119,
      "page_num": 9,
      "paragraph_text": "def cosine_similarity(vec1, vec2):"
    },
    {
      "paragraph_id": 120,
      "page_num": 9,
      "paragraph_text": "return np.dot(vec1, vec2) / (np.linalg.norm(vec1) *"
    },
    {
      "paragraph_id": 121,
      "page_num": 9,
      "paragraph_text": "np.linalg.norm(vec2)) ↪"
    },
    {
      "paragraph_id": 122,
      "page_num": 9,
      "paragraph_text": "Den här funktionen kan vi nu använda för att utföra en semantisk sökning bland våra embeddings."
    },
    {
      "paragraph_id": 123,
      "page_num": 9,
      "paragraph_text": "337"
    },
    {
      "paragraph_id": 124,
      "page_num": 10,
      "paragraph_text": "def semantic_search(query, chunks, embeddings, k=5):"
    },
    {
      "paragraph_id": 125,
      "page_num": 10,
      "paragraph_text": "query_embedding = create_embeddings(query).embeddings[0].values 1 ↪"
    },
    {
      "paragraph_id": 126,
      "page_num": 10,
      "paragraph_text": "similarity_scores = []"
    },
    {
      "paragraph_id": 127,
      "page_num": 10,
      "paragraph_text": "for i, chunk_embedding in enumerate(embeddings.embeddings):"
    },
    {
      "paragraph_id": 128,
      "page_num": 10,
      "paragraph_text": "similarity_score = cosine_similarity("
    },
    {
      "paragraph_id": 129,
      "page_num": 10,
      "paragraph_text": "query_embedding, chunk_embedding.values ) similarity_scores.append((i, similarity_score))"
    },
    {
      "paragraph_id": 130,
      "page_num": 10,
      "paragraph_text": "similarity_scores.sort(key=lambda x: x[1], reverse=True) top_indices = [index for index, _ in similarity_scores[:k]]"
    },
    {
      "paragraph_id": 131,
      "page_num": 10,
      "paragraph_text": "return [chunks[index] for index in top_indices]"
    },
    {
      "paragraph_id": 132,
      "page_num": 10,
      "paragraph_text": "1 embed_content() returnerar ett EmbedContentResponse-objekt som innehåller oli- ka data. Vi är bara intresserade av själva värdena på våra embeddings, vilka vi hittar i embeddings-attributet."
    },
    {
      "paragraph_id": 133,
      "page_num": 10,
      "paragraph_text": "Vi får ut de fem chunks som har störst semantisk likhet med vår query. Vi kollar på det första resultatet."
    },
    {
      "paragraph_id": 134,
      "page_num": 10,
      "paragraph_text": "Fråga: Vad innebär chunking?\nSvar:\nka data. Vi är bara intresserade av själva värdena på våra embeddings, vilka vi hittar i embeddings-attributet.\nVi får ut de fem chunks som har störst semantisk likhet med vår query.\nVi kollar på det första resultatet.\nquery = \"Vad innebär chunking?\" print(f\"Fråga: {query}\") print(\"Svar:\") print(semantic_search(query, chunks, embeddings)[0]) query = \"Vad innebär chunking?\" print(f\"Fråga: {query}\") print(\"Svar:\") print(semantic_search(query, chunks, embeddings)[0])"
    },
    {
      "paragraph_id": 135,
      "page_num": 10,
      "paragraph_text": "338"
    },
    {
      "paragraph_id": 136,
      "page_num": 11,
      "paragraph_text": "Fråga: Vad innebär chunking?\nSvar:\nvi hittar i embeddings-attributet.\nVi får ut de fem chunks som har störst semantisk likhet med vår query.\nVi kollar på det första resultatet.\nquery = \"Vad innebär chunking?\" print(f\"Fråga: {query}\") print(\"Svar:\") print(semantic_search(query, chunks, embeddings)[0]) Fråga: Vad innebär chunking?\nSvar:\n12som innehåller olika data. Vi är bara intresserade av själva värdena på våra embeddings, vilka vi hittar i embeddings-attributet.\nVi får ut de fem chunks som har störst semantisk likhet med vår query.\nVi kollar på det första resultatet.\nquery = \"Vad innebär chunking?\" print(f\"Fråga: {query}\") print(\"Svar:\") print"
    },
    {
      "paragraph_id": 137,
      "page_num": 11,
      "paragraph_text": "Ĺ Semantic chunking"
    },
    {
      "paragraph_id": 138,
      "page_num": 11,
      "paragraph_text": "Nu när vi känner till begreppet semantisk sökning kan vi nämna ytterligare en metod för chunking, nämligen semantic chunking."
    },
    {
      "paragraph_id": 139,
      "page_num": 11,
      "paragraph_text": "Semantic chunking innebär att vi börjar med en sentence based chunking. Se- dan skapar vi embeddings av våra meningar och jämför deras innehåll med en semantisk sökning. Utifrån resultaten skapar vi sedan chunks som innehåller meningar som ligger nära varandra i betydelse. Det är en bra metod om vi har flera olika dokument som tar upp samma saker."
    },
    {
      "paragraph_id": 140,
      "page_num": 11,
      "paragraph_text": "339"
    },
    {
      "paragraph_id": 141,
      "page_num": 12,
      "paragraph_text": "10.4.5 Generera svar"
    },
    {
      "paragraph_id": 142,
      "page_num": 12,
      "paragraph_text": "Vår semantiska sökning verkar fungera ganska bra, men den återger ju bara texten så som den står i kapitlet. Nu är vi redo att låta språkmodellen generera svar som låter mer naturliga."
    },
    {
      "paragraph_id": 143,
      "page_num": 12,
      "paragraph_text": "Vi börjar med att definiera en system prompt, som talar om för språkmodellen att den bara får utgå från informationen i kontexten som vi kommer skicka med när vi ställer själva frågan i vår user prompt."
    },
    {
      "paragraph_id": 144,
      "page_num": 12,
      "paragraph_text": "system_prompt = \"\"\"Jag kommer ställa dig en fråga, och jag vill att du svarar ↪ baserat bara på kontexten jag skickar med, och ingen annan information.\n↪ Om det inte finns nog med information i kontexten för att svara på frågan, ↪ säg \"Det vet jag inte\". Försök inte att gissa.\nFormulera dig enkelt och dela upp svaret i fina stycken. \"\"\""
    },
    {
      "paragraph_id": 145,
      "page_num": 12,
      "paragraph_text": "Nu kan vi definiera en funktion, generate_user_prompt(), som tar en fråga, skapar en kontext genom en semantisk sökning med vår fråga, och bakar ihop allting till en enda prompt som vi kan skicka till språkmodellen."
    },
    {
      "paragraph_id": 146,
      "page_num": 12,
      "paragraph_text": "def generate_user_prompt(query):"
    },
    {
      "paragraph_id": 147,
      "page_num": 12,
      "paragraph_text": "context = \"\\n\".join(semantic_search(query, chunks, embeddings)) ↪"
    },
    {
      "paragraph_id": 148,
      "page_num": 12,
      "paragraph_text": "user_prompt = f\"Frågan är {query}. Här är kontexten:\n{context}.\" ↪"
    },
    {
      "paragraph_id": 149,
      "page_num": 12,
      "paragraph_text": "return user_prompt"
    },
    {
      "paragraph_id": 150,
      "page_num": 12,
      "paragraph_text": "Nu kan vi ställa en fråga till språkmodellen. Vi definierar ytterligare en funktion, generate_response()."
    },
    {
      "paragraph_id": 151,
      "page_num": 12,
      "paragraph_text": "def generate_response(system_prompt, user_message, model=\"gemini-2.0-flash\"):\n↪"
    },
    {
      "paragraph_id": 152,
      "page_num": 12,
      "paragraph_text": "response = client.models.generate_content("
    },
    {
      "paragraph_id": 153,
      "page_num": 12,
      "paragraph_text": "340"
    },
    {
      "paragraph_id": 154,
      "page_num": 13,
      "paragraph_text": "model=model, config=types.GenerateContentConfig("
    },
    {
      "paragraph_id": 155,
      "page_num": 13,
      "paragraph_text": "system_instruction=system_prompt), contents=generate_user_prompt(user_message) ) return response"
    },
    {
      "paragraph_id": 156,
      "page_num": 13,
      "paragraph_text": "print(generate_response(system_prompt, \"Vad är semantic chunking?\").text) ↪"
    },
    {
      "paragraph_id": 157,
      "page_num": 13,
      "paragraph_text": "Semantic chunking är en metod för chunking som börjar med sentence based chunking. Sedan skapas embeddings av meningarna och deras innehåll jämförs med en semantisk sökning. Utifrån resultaten skapas sedan chunks som innehåller meningar som ligger nära varandra i betydelse."
    },
    {
      "paragraph_id": 158,
      "page_num": 13,
      "paragraph_text": "10.4.6 Evaluering"
    },
    {
      "paragraph_id": 159,
      "page_num": 13,
      "paragraph_text": "Hur vet vi om vår RAG-modell fungerar som vi vill? Vi kan naturligtvis ställa den ett antal frågor och själva bilda oss en uppfattning, men det är ofta en bättre idé att skapa ett system för att utvärdera modellen med kod."
    },
    {
      "paragraph_id": 160,
      "page_num": 13,
      "paragraph_text": "Vi skriver ett antal frågor, samt svar på frågorna som är i linje med vad vi önskar att modellen svarar. Sedan kan vi med hjälp av en annan system prompt låta modellen utvärdera om svaren på frågorna är i linje med det vi önskar, och sätta “betyg” på svaren. Det ger oss data att jämföra olika modeller med."
    },
    {
      "paragraph_id": 161,
      "page_num": 13,
      "paragraph_text": "Vi börjar med att skriva ett antal testfrågor, och de svar vi önskar att modellen ska ge oss. En testfråga kan se ut som i exemplet nedan."
    },
    {
      "paragraph_id": 162,
      "page_num": 13,
      "paragraph_text": "validation_data = ["
    },
    {
      "paragraph_id": 163,
      "page_num": 13,
      "paragraph_text": "{"
    },
    {
      "paragraph_id": 164,
      "page_num": 13,
      "paragraph_text": "\"question\": \"Vilka delar utgör en RAG-modell?\", \"ideal_answer\": \"En RAG-modell innehåller två delar: en retriever som söker efter relevanta stycken i en text, och en generator som genererar svar utifrån den givna kontexten.\""
    },
    {
      "paragraph_id": 165,
      "page_num": 13,
      "paragraph_text": "↪"
    },
    {
      "paragraph_id": 166,
      "page_num": 13,
      "paragraph_text": "↪"
    },
    {
      "paragraph_id": 167,
      "page_num": 13,
      "paragraph_text": "↪ }"
    },
    {
      "paragraph_id": 168,
      "page_num": 13,
      "paragraph_text": "341"
    },
    {
      "paragraph_id": 169,
      "page_num": 14,
      "paragraph_text": "]"
    },
    {
      "paragraph_id": 170,
      "page_num": 14,
      "paragraph_text": "validation_data[0][\"question\"]"
    },
    {
      "paragraph_id": 171,
      "page_num": 14,
      "paragraph_text": "'Vilka delar utgör en RAG-modell?'"
    },
    {
      "paragraph_id": 172,
      "page_num": 14,
      "paragraph_text": "validation_data[0][\"ideal_answer\"]"
    },
    {
      "paragraph_id": 173,
      "page_num": 14,
      "paragraph_text": "'En RAG-modell innehåller två delar: en retriever som söker efter relevanta stycken i en text, och en generator som generar svar utifrån den givna kontexten.'"
    },
    {
      "paragraph_id": 174,
      "page_num": 14,
      "paragraph_text": "Nu skriver vi en ny system prompt som talar om för språkmodellen att vi vill att den ska utvärdera svaret."
    },
    {
      "paragraph_id": 175,
      "page_num": 14,
      "paragraph_text": "evaluation_system_prompt = \"\"\"Du är ett intelligent utvärderingssystem vars uppgift är att utvärdera en AI-assistents svar."
    },
    {
      "paragraph_id": 176,
      "page_num": 14,
      "paragraph_text": "↪"
    },
    {
      "paragraph_id": 177,
      "page_num": 14,
      "paragraph_text": "↪ Om svaret är väldigt nära det önskade svaret, sätt poängen 1. Om svaret är felaktigt eller inte bra nog, sätt poängen 0.\n↪ Om svaret är delvis i linje med det önskade svaret, sätt poängen 0.5. Motivera kort varför du sätter den poäng du gör.\n↪ \"\"\""
    },
    {
      "paragraph_id": 178,
      "page_num": 14,
      "paragraph_text": "query = validation_data[0][\"question\"] response = generate_response(system_prompt, query)"
    },
    {
      "paragraph_id": 179,
      "page_num": 14,
      "paragraph_text": "evaluation_prompt = f\"\"\"Fråga: {query} AI-assistentens svar: {response.text} Önskat svar: {validation_data[0]['ideal_answer']}\"\"\""
    },
    {
      "paragraph_id": 180,
      "page_num": 14,
      "paragraph_text": "evaluation_response = generate_response(evaluation_system_prompt, evaluation_prompt) ↪"
    },
    {
      "paragraph_id": 181,
      "page_num": 14,
      "paragraph_text": "print(evaluation_response.text)"
    },
    {
      "paragraph_id": 182,
      "page_num": 14,
      "paragraph_text": "342"
    },
    {
      "paragraph_id": 183,
      "page_num": 15,
      "paragraph_text": "Poäng: 1 Motivering: Svaret är korrekt och relevant i förhållande till frågan samt följer instruktionerna."
    },
    {
      "paragraph_id": 184,
      "page_num": 15,
      "paragraph_text": "Vi kan också ge modellen en fråga som ligger utanför den givna kontexten och försäkra oss om att den inte försöker hitta på ett svar."
    },
    {
      "paragraph_id": 185,
      "page_num": 15,
      "paragraph_text": "validation_data[2]"
    },
    {
      "paragraph_id": 186,
      "page_num": 15,
      "paragraph_text": "{'question': 'Hur många bultar finns det i Ölandsbron?', 'ideal_answer': 'Det vet jag inte.'}"
    },
    {
      "paragraph_id": 187,
      "page_num": 15,
      "paragraph_text": "query = validation_data[2][\"question\"] response = generate_response(system_prompt, query)"
    },
    {
      "paragraph_id": 188,
      "page_num": 15,
      "paragraph_text": "evaluation_prompt = f\"\"\"Fråga: {query} AI-assistentens svar: {response.text} Önskat svar: {validation_data[2]['ideal_answer']}\"\"\""
    },
    {
      "paragraph_id": 189,
      "page_num": 15,
      "paragraph_text": "evaluation_response = generate_response(evaluation_system_prompt, evaluation_prompt) ↪"
    },
    {
      "paragraph_id": 190,
      "page_num": 15,
      "paragraph_text": "print(evaluation_response.text)"
    },
    {
      "paragraph_id": 191,
      "page_num": 15,
      "paragraph_text": "Poäng: 1 Motivering: AI-assistentens svar är korrekt och i linje med det önskade svaret, vilket indikerar att den följer instruktionerna om att svara \"Det vet jag inte\" när information saknas i kontexten."
    },
    {
      "paragraph_id": 192,
      "page_num": 15,
      "paragraph_text": "10.5 Vector store"
    },
    {
      "paragraph_id": 193,
      "page_num": 15,
      "paragraph_text": "I det här kapitlet har vi arbetat med en liten mängd data. I fall där kontexten består av många dokument, bilder, filmer eller ljudfiler kan det ta lång tid att generera embeddings.\nDå är det en bra idé att spara dem till en fil. Det gör det också möjligt att dela våra embeddings med andra, eller att använda en molntjänst för att generera dem och sedan ladda ned dem och arbeta med dem lokalt på vår egen dator."
    },
    {
      "paragraph_id": 194,
      "page_num": 15,
      "paragraph_text": "Det är vanligt att man använder en vektordatabas för att lagra embeddings. Exempel på vektordatabaser är faiss, qdrant och ChromaDB."
    },
    {
      "paragraph_id": 195,
      "page_num": 15,
      "paragraph_text": "343"
    },
    {
      "paragraph_id": 196,
      "page_num": 16,
      "paragraph_text": "För enklare projekt är det dock inte nödvändigt att använda en vektordatabas. Vi kan implementera en enkel vector store med numpy, och använda datahanteringsbiblioteket polars för att spara våra embeddings, tillsammans med deras metadata, i Parquet- format. Vi använder polars istället för pandas på grund av att polars har bättre stöd för nästlad data, alltså kolumner som innehåller exempelvis listor."
    },
    {
      "paragraph_id": 197,
      "page_num": 16,
      "paragraph_text": "import polars as pl"
    },
    {
      "paragraph_id": 198,
      "page_num": 16,
      "paragraph_text": "class VectorStore:"
    },
    {
      "paragraph_id": 199,
      "page_num": 16,
      "paragraph_text": "def __init__(self):"
    },
    {
      "paragraph_id": 200,
      "page_num": 16,
      "paragraph_text": "self.vectors = [] self.texts = [] self.metadata = []"
    },
    {
      "paragraph_id": 201,
      "page_num": 16,
      "paragraph_text": "def add_item(self, text, embedding, metadata=None):"
    },
    {
      "paragraph_id": 202,
      "page_num": 16,
      "paragraph_text": "self.vectors.append(np.array(embedding)) self.texts.append(text) self.metadata.append(metadata or {})"
    },
    {
      "paragraph_id": 203,
      "page_num": 16,
      "paragraph_text": "def semantic_search(self, query_embedding, k=5):"
    },
    {
      "paragraph_id": 204,
      "page_num": 16,
      "paragraph_text": "if not self.vectors:"
    },
    {
      "paragraph_id": 205,
      "page_num": 16,
      "paragraph_text": "return []"
    },
    {
      "paragraph_id": 206,
      "page_num": 16,
      "paragraph_text": "query_vector = np.array(query_embedding)"
    },
    {
      "paragraph_id": 207,
      "page_num": 16,
      "paragraph_text": "similarities = [] for i, vector in enumerate(self.vectors):"
    },
    {
      "paragraph_id": 208,
      "page_num": 16,
      "paragraph_text": "similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector)) 1 ↪"
    },
    {
      "paragraph_id": 209,
      "page_num": 16,
      "paragraph_text": "similarities.append((i, similarity))"
    },
    {
      "paragraph_id": 210,
      "page_num": 16,
      "paragraph_text": "similarities.sort(key=lambda x: x[1], reverse=True)"
    },
    {
      "paragraph_id": 211,
      "page_num": 16,
      "paragraph_text": "results = [] for i in range(min(k, len(similarities))):"
    },
    {
      "paragraph_id": 212,
      "page_num": 16,
      "paragraph_text": "idx, score = similarities[i] results.append({"
    },
    {
      "paragraph_id": 213,
      "page_num": 16,
      "paragraph_text": "\"text\": self.texts[idx],"
    },
    {
      "paragraph_id": 214,
      "page_num": 16,
      "paragraph_text": "344"
    },
    {
      "paragraph_id": 215,
      "page_num": 17,
      "paragraph_text": "\"metadata\": self.metadata[idx], \"similarity\": score })"
    },
    {
      "paragraph_id": 216,
      "page_num": 17,
      "paragraph_text": "return results"
    },
    {
      "paragraph_id": 217,
      "page_num": 17,
      "paragraph_text": "def save(self):"
    },
    {
      "paragraph_id": 218,
      "page_num": 17,
      "paragraph_text": "df = pl.DataFrame("
    },
    {
      "paragraph_id": 219,
      "page_num": 17,
      "paragraph_text": "dict("
    },
    {
      "paragraph_id": 220,
      "page_num": 17,
      "paragraph_text": "vectors=self.vectors, texts=self.texts, metadata=self.metadata) ) df.write_parquet(\"embeddings.parquet\")"
    },
    {
      "paragraph_id": 221,
      "page_num": 17,
      "paragraph_text": "def load(self, file):"
    },
    {
      "paragraph_id": 222,
      "page_num": 17,
      "paragraph_text": "df = pl.read_parquet(file, columns=[\"vectors\", \"texts\", \"metadata\"]) ↪"
    },
    {
      "paragraph_id": 223,
      "page_num": 17,
      "paragraph_text": "self.vectors = df[\"vectors\"].to_list() self.texts = df[\"texts\"].to_list() self.metadata = df[\"metadata\"].to_list()"
    },
    {
      "paragraph_id": 224,
      "page_num": 17,
      "paragraph_text": "1 Det här är cosine_similarity()-funktionen från tidigare."
    },
    {
      "paragraph_id": 225,
      "page_num": 17,
      "paragraph_text": "Nu kan vi lagra våra embeddings i en instans av vår VectorStore-klass."
    },
    {
      "paragraph_id": 226,
      "page_num": 17,
      "paragraph_text": "vector_store = VectorStore()"
    },
    {
      "paragraph_id": 227,
      "page_num": 17,
      "paragraph_text": "for i, chunk in enumerate(chunks):"
    },
    {
      "paragraph_id": 228,
      "page_num": 17,
      "paragraph_text": "chunk_embedding = create_embeddings(chunk).embeddings[0].values ↪"
    },
    {
      "paragraph_id": 229,
      "page_num": 17,
      "paragraph_text": "vector_store.add_item("
    },
    {
      "paragraph_id": 230,
      "page_num": 17,
      "paragraph_text": "text=chunk, embedding=chunk_embedding, metadata={\"type\": \"chunk\", \"index\": i} )"
    },
    {
      "paragraph_id": 231,
      "page_num": 17,
      "paragraph_text": "Vi använder metoden semantic_search() för att utföra semantiska sökningar bland våra chunks."
    },
    {
      "paragraph_id": 232,
      "page_num": 17,
      "paragraph_text": "345"
    },
    {
      "paragraph_id": 233,
      "page_num": 18,
      "paragraph_text": "query = \"Vad är en semantisk sökning?\" query_embedding = create_embeddings(query).embeddings[0].values"
    },
    {
      "paragraph_id": 234,
      "page_num": 18,
      "paragraph_text": "vector_store.semantic_search(query_embedding=query_embedding)"
    },
    {
      "paragraph_id": 235,
      "page_num": 18,
      "paragraph_text": "Med metoden save() kan vi lagra våra embeddings, tillsammans med texten och meta- datan. Filen embeddings.parquet kan vi sedan till exempel dela med oss av till andra som vill använda våra embeddings."
    },
    {
      "paragraph_id": 236,
      "page_num": 18,
      "paragraph_text": "vector_store.save()"
    },
    {
      "paragraph_id": 237,
      "page_num": 18,
      "paragraph_text": "Metoden load() låter oss läsa in en lagrad fil."
    },
    {
      "paragraph_id": 238,
      "page_num": 18,
      "paragraph_text": "vector_store2 = VectorStore() vector_store2.load(\"embeddings.parquet\")"
    },
    {
      "paragraph_id": 239,
      "page_num": 18,
      "paragraph_text": "Vår vector store kan vi nu använda istället för att generera kontext som vi kan skicka med till språkmodellen."
    },
    {
      "paragraph_id": 240,
      "page_num": 18,
      "paragraph_text": "10.6 Vidare arbete med chattbottar"
    },
    {
      "paragraph_id": 241,
      "page_num": 18,
      "paragraph_text": "Det här kapitlet har gett en introduktion till hur vi bygger egna chattbottar. Den läsare som vill fördjupa sig och bland annat använda etablerade och välkända bibliotek kan undersöka till exempel LangChain för att hantera språkmodeller, och DeepEval för att evaluera dem."
    },
    {
      "paragraph_id": 242,
      "page_num": 18,
      "paragraph_text": "10.7 Övningsuppgifter"
    },
    {
      "paragraph_id": 243,
      "page_num": 18,
      "paragraph_text": "Övningsuppgifter för kapitlet finns på bokens hemsida:\nhttps://github.com/AntonioPrgomet/ai_tillaempad_ml"
    },
    {
      "paragraph_id": 244,
      "page_num": 18,
      "paragraph_text": "346"
    }
  ]
}